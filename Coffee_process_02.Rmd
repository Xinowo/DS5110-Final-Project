---
title: "Coffee_review_processed_02"
author: "Henry Guo"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
library(formatR)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, tidy.opts = list(width.cutoff = 300))
```

```{r}
library("easypackages")                                             
libraries("tidyverse","ggthemes",
          "ggplot2", "scales", "dplyr", "readr", "stringr","tidytext",
          "tidyr", "DBI", "RSQLite")
coffee <- read_csv("~/Desktop/coffee_review_processed_01.csv")
```

Finding most popular Coffee Origins
```{r}
coffee <- coffee %>%
  mutate(coffee_origin_lower = tolower(`Coffee Origin`)) %>% 
  mutate(roast_level = tolower(`Roast Level`))

top_words <- coffee %>%
  unnest_tokens(word, coffee_origin_lower) %>%
  count(word, sort = TRUE) %>%
  slice_head(n = 100)
#top_words
```

Here are the countries / regions where it appeared more than 100 times:ethiopia, columbia, kenya, guatemala, yirgacheffe (ethiopia), indonesia, costa rica, sumatra (indonesia), brazil, panama, sidamo (ethiopia), oromia(ethiopia), guji(ethiopia), salvador, boquete(panama), nyeri (kenya), huila (columbia), kona (hawaii), rwanda, cauca (columb), sidama (ethiopia), honduras, huehuetenango (guatemala), nicaragua, peru, hawaii, hawai'i, mexico, tarrazu (costa rica), gerais (brazil), guinea, papua (guinea), aceh (indonesia), antigua

Below are coffee_origin categorized. Please adjust if you see fit.
```{r}
coffee <- coffee %>%
  mutate(coffee_origin = str_replace_all(coffee_origin_lower, "[^[:alnum:]]", "")) %>% 
  mutate(coffee_origin_adj = case_when(
    str_detect(coffee_origin, "colomb") ~ "colombia",
    str_detect(coffee_origin, "brazil") ~ "brazil",
    str_detect(coffee_origin, "guatem") ~ "guatemala",
    str_detect(coffee_origin, "kenya") ~ "kenya",
    str_detect(coffee_origin, "ethiopia") ~ "ethiopia",
    str_detect(coffee_origin, "yirgacheffe") ~ "ethiopia",
    str_detect(coffee_origin, "indo") ~ "indonesia",
    str_detect(coffee_origin, "costa") ~ "costa rica",
    str_detect(coffee_origin, "rica") ~ "costa rica",
    str_detect(coffee_origin, "sumatra") ~ "indonesia",
    str_detect(coffee_origin, "sidamo") ~ "ethiopia",
    str_detect(coffee_origin, "oromia") ~ "ethiopia",
    str_detect(coffee_origin, "guji") ~ "ethiopia",
    str_detect(coffee_origin, "salvador") ~ "salvador",
    str_detect(coffee_origin, "boquete") ~ "panama",
    str_detect(coffee_origin, "panama") ~ "panama",
    str_detect(coffee_origin, "nyeri") ~ "kenya",
    str_detect(coffee_origin, "huila") ~ "colombia",
    str_detect(coffee_origin, "kona") ~ "hawaii",
    str_detect(coffee_origin, "rwan") ~ "rwanda",
    str_detect(coffee_origin, "cauca") ~ "colombia",
    str_detect(coffee_origin, "hondu") ~ "honduras",
    str_detect(coffee_origin, "huehuetenango") ~ "guatemala",
    str_detect(coffee_origin, "nicaragua") ~ "nicaragua",
    str_detect(coffee_origin, "peru") ~ "peru",
    str_detect(coffee_origin, "hawa") ~ "hawaii",
    str_detect(coffee_origin, "mexico") ~ "mexico",
    str_detect(coffee_origin, "tarrazu") ~ "costa rica",
    str_detect(coffee_origin, "gerais") ~ "brazil",
    str_detect(coffee_origin, "guinea") ~ "australia",
    str_detect(coffee_origin, "australia") ~ "australia",
    str_detect(coffee_origin, "papua") ~ "australia",
    str_detect(coffee_origin, "aceh") ~ "indonesia",
    str_detect(coffee_origin, "antigua") ~ "antigua",
    str_detect(coffee_origin, "cong") ~ "congo",
    str_detect(coffee_origin, "puert") ~ "peurto rico",
    str_detect(coffee_origin, "yemen") ~ "yemen",
    str_detect(coffee_origin, "taiwan") ~ "taiwan",
    str_detect(coffee_origin, "china") ~ "china",
    str_detect(coffee_origin, "india") ~ "india",
    str_detect(coffee_origin, "philippine") ~ "philippines",
    str_detect(coffee_origin, "jamaica") ~ "jamaica",
    str_detect(coffee_origin, "bolivia") ~ "bolivia",
    str_detect(coffee_origin, "haiti") ~ "haiti",
    str_detect(coffee_origin, "kayanza") ~ "burundi",
    str_detect(coffee_origin, "burundi") ~ "burundi",
    str_detect(coffee_origin, "thai") ~ "thailand",
    str_detect(coffee_origin, "viet") ~ "vietnam",
    str_detect(coffee_origin, "cali") ~ "united states",
    str_detect(coffee_origin, "tanzania") ~ "tanzania",
    str_detect(coffee_origin, "ecuador") ~ "ecuador",
    str_detect(coffee_origin, "uganda") ~ "uganda",
    str_detect(coffee_origin, "zimbab") ~ "zimbabwe",
    str_detect(coffee_origin, "zambia") ~ "zambia",
    str_detect(coffee_origin, "dominic") ~ "dominican republic",
    str_detect(coffee_origin, "disclose") ~ "NA",
    str_detect(coffee_origin, "NA") ~ "NA",
    #TRUE ~ coffee_origin
    TRUE ~ "others"
  ))
```

Roast Level Adjustment
```{r}
coffee <- coffee %>%
  mutate(roast_lv_adj = case_when(
    roast_level == "very dark" ~ 6,
    roast_level == "dark" ~ 5,
    roast_level == "medium-dark" ~ 4,
    roast_level == "medium" ~ 3,
    roast_level == "medium-light" ~ 2,
    roast_level == "light" ~ 1,
    TRUE ~ NA_real_ 
  ))
```
Coffee name with 100%
```{r}
coffee <- coffee %>%
  mutate(coffee_name_hundredpercent =str_detect(`Coffee Name`, "100"))
```

Review data from character to date-time.
```{r}
coffee <- coffee %>%
  mutate(review_date_adj = as.Date(paste("01", `Review Date`), format = "%d %B %Y"))
```

```{r}
coffee <- coffee %>% #for more convenient typing
  rename(A=`Blind Assessment`, B=`Notes`, C=`Who Should Drink It`, D=`Bottom Line`)

coffee <- coffee %>% 
  select(-`all_text`, -`Agtron`,-`Est. Price`,-`Roast Level`, -`roast_level`,-`coffee_origin_lower`,-`coffee_origin`,-`Est. Price`,-`Review Date`)
write_csv(coffee, "~/Desktop/coffee_ultimato.csv")
```

Word Frequency Raw Count
```{r}
top_words_by_column <- coffee %>%
  pivot_longer(cols = c(A, B, C, D), names_to = "column", values_to = "text") %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words, by = "word") %>%
  count(column, word, sort = TRUE) %>%
  group_by(column) %>%
  slice_max(order_by = n, n = 20) %>%
  ungroup()

ggplot(top_words_by_column, aes(x = reorder(word, n), y = n)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ column, scales = "free_y") +
  labs(x = "Word", y = "Frequency", title = "Top 30 Words per Column") +
  coord_flip()+
  theme(axis.text.y = element_text(angle = 325, size = 5.7))
```
Word Frequency TF-IDF
```{r}
tfidf_data <- coffee %>%
  pivot_longer(cols = c(A, B, C, D),
               names_to = "document",
               values_to = "text") %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words, by = "word") %>%
  filter(!(document == "B" & str_detect(word, "\\d"))) %>%
  count(document, word, sort = TRUE) %>%
  bind_tf_idf(word, document, n)

tfidf_top <- tfidf_data %>%
  group_by(document) %>%
  top_n(20, tf_idf) %>%
  ungroup() %>%
  arrange(document, -tf_idf)

ggplot(tfidf_top, aes(x = reorder_within(word, tf_idf, document), y = tf_idf, fill = document)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ document, scales = "free_y") +
  coord_flip() +
  scale_x_reordered() +
  labs(x = "Word", y = "TF-IDF", title = "Top TF-IDF Words per Column") +
  theme(axis.text.y = element_text(angle = 335, size = 6.5))
```

```{r}
coffee <- coffee %>%
  mutate(review_id = row_number())

#Pivot the comment columns into long format.
coffee_long <- coffee %>%
  pivot_longer(cols = c(A, B, C, D),
               names_to = "document",
               values_to = "text")

tokens <- coffee_long %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words, by = "word")

token_counts <- tokens %>%
  count(review_id, document, word, sort = TRUE)

tfidf_data <- token_counts %>%
  bind_tf_idf(word, review_id, n)

# Summarize text features per review and per comment type.
review_features <- tfidf_data %>%
  group_by(review_id, document) %>%
  summarise(avg_tf_idf = mean(tf_idf), .groups = "drop")

coffee_features <- coffee %>%
  select(review_id, Rating) %>%
  left_join(review_features, by = "review_id")

ggplot(coffee_features, aes(x = avg_tf_idf, y = Rating)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm") +
  facet_wrap(~ document) +
  labs(x = "Average TF-IDF", y = "Rating", title = "Rating vs Average TF-IDF by Comment Column") +
  theme_minimal()
```
TF-IDF per review per comment type creates a numeric value representing the importance of words in the comments. We see that in A,B,C, reviewers tend to give lower ratings when using more distinctive language in the comment, and vice versa.

```{r}
coffee <- coffee %>%
  mutate(review_id = row_number())

coffee_long <- coffee %>%
  pivot_longer(cols = c(A, B, C, D),
               names_to = "document",
               values_to = "text")

tokens <- coffee_long %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words, by = "word")

tokens <- tokens %>%
  filter(!(document == "B" & str_detect(word, "\\d")))

token_counts <- tokens %>%
  count(review_id, document, word, sort = TRUE)

tfidf_data <- token_counts %>%
  bind_tf_idf(word, review_id, n)

review_features <- tfidf_data %>%
  group_by(review_id, document) %>%
  summarise(avg_tf_idf = mean(tf_idf), .groups = "drop")

coffee_features <- coffee %>%
  select(review_id, Rating) %>%
  left_join(review_features, by = "review_id")

#  correlation analysis.
cor_results <- coffee_features %>%
  group_by(document) %>%
  summarise(correlation = cor(avg_tf_idf, Rating, use = "complete.obs"))

print(cor_results)
```
Reviewers tend to give harsher ratings when using distinctive words in the order of A > B > C >D

LDA: (this takes a while to run)
Identify latent topics in the reviews to understand common themes.
```{r}
library(topicmodels)
# Create a document-term matrix from long-format data
documents <- coffee %>%
  pivot_longer(cols = c(A, B, C, D),
               names_to = "document",
               values_to = "text") %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words, by = "word") %>%
  count(document, review_id, word, sort = TRUE) %>%
  ungroup()

documents <- documents %>%
  mutate(doc_id = paste(document, review_id, sep = "_"))

# cast to a sparse matrix
dtm <- documents %>%
  cast_dtm(document = doc_id, term = word, value = n)

lda_model <- LDA(dtm, k = 7, control = list(seed = 1234))

# Extract topics per document
topics <- tidy(lda_model, matrix = "beta")
top_terms <- topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  arrange(topic, -beta)

print(top_terms)
```
Beta: estimated probability of a word given a topic
Notable words
k = 5: sweet, chocolate, fruit
k = 3: sweet, fruit
k = 7: chocolate, fruit

N-gram Analysis
Investigate bigrams & trigrams to capture context that individual words might miss.
```{r}
bigram <- coffee %>%
  pivot_longer(cols = c(A, B, C, D), names_to = "document", values_to = "text") %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, into = c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word) %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(document, bigram, sort = TRUE) %>%
  group_by(document) %>%
  slice_max(order_by = n, n = 15) %>%
  ungroup()

ggplot(bigram, aes(x = reorder(bigram, n), y = n)) +
  geom_col(fill = "steelblue") +
  facet_wrap(~ document, scales = "free_y") +
  coord_flip() +
  labs(title = "Top 15 Bigrams per Column", x = "Bigram", y = "Frequency") +
  theme(axis.text.y = element_text(angle = 335, size = 5))
```
```{r}
trigrams <- coffee %>%
  pivot_longer(cols = c(A, B, C, D), 
               names_to = "document", 
               values_to = "text") %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, into = c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  unite(trigram, word1, word2, word3, sep = " ") %>%
  count(document, trigram, sort = TRUE) %>%
  group_by(document) %>%
  slice_max(order_by = n, n = 15) %>%
  ungroup()

ggplot(trigrams, aes(x = reorder(trigram, n), y = n)) +
  geom_col(fill = "darkgreen") +
  facet_wrap(~ document, scales = "free_y") +
  coord_flip() +
  labs(title = "Top 15 Trigrams per Column",
       x = "Trigram", y = "Frequency") +
  theme(axis.text.y = element_text(angle = 335, size = 5))
```

Temporal Analysis
```{r}
temporal_sentiment <- coffee %>%
  mutate(year = lubridate::year(review_date_adj)) %>%
  left_join(sentiment_bing, by = "review_id") %>%
  group_by(year) %>%
  summarise(avg_sentiment = mean(net_sentiment, na.rm = TRUE))

ggplot(temporal_sentiment, aes(x = year, y = avg_sentiment)) +
  geom_line() +
  geom_point() +
  labs(title = "Temporal Trend of Average Sentiment",
       x = "Year", y = "Average Net Sentiment") +
  theme_minimal()
```
Reviews have generally became more positive.


Sentiment Analysis: Bing Lexicon
Compare sentiment scores calculated with different lexicons to see which one correlates best with review Rating and how the sentiments differ by comment column.
```{r}
# Calculate Bing sentiment scores and net sentiment:
sentiment_bing <- coffee %>%
  pivot_longer(cols = c(A, B, C, D), 
               names_to = "document", 
               values_to = "text") %>%
  unnest_tokens(word, text) %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(review_id, document, sentiment) %>%
  pivot_wider(names_from = sentiment, 
              values_from = n, 
              values_fill = list(n = 0)) %>%
  mutate(net_sentiment = positive - negative)

# Join with the Rating column:
sentiment_data <- coffee %>%
  select(review_id, Rating) %>%
  left_join(sentiment_bing, by = "review_id")

# Create a faceted plot for all documents in one figure:
ggplot(sentiment_data, aes(x = net_sentiment, y = Rating)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~ document) +
  labs(title = "Sentiment vs. Rating (Bing Lexicon)",
       x = "Net Sentiment (Positive - Negative)", 
       y = "Rating") +
  theme_minimal()
```


Sentiment Analysis: AFINN
Provides numeric sentiment scores for words
Get a continuous sentiment score that tells how strongly positive or negative the text is.
```{r}
afinn_sentiment <- coffee %>%
  pivot_longer(cols = c(A, B, C, D),
               names_to = "document",
               values_to = "text") %>%
  unnest_tokens(word, text) %>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  group_by(review_id, document) %>%
  summarise(net_sentiment = sum(value, na.rm = TRUE), .groups = "drop")

afinn_sentiment_with_rating <- afinn_sentiment %>%
  left_join(coffee %>% select(review_id, Rating), by = "review_id")

ggplot(afinn_sentiment_with_rating, aes(x = net_sentiment, y = Rating)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm") +
  facet_wrap(~ document) +
  labs(title = "AFINN Net Sentiment vs. Rating by Document",
       x = "Net AFINN Sentiment", y = "Rating") +
  theme_minimal()
```
A positive net sentiment means that, overall, the words in that review contribute positive values and vice versa.

Sentiment Analysis NRC
Assigns words to multiple emotion categories
Explore the mix of emotions present in the reviews.
```{r}
nrc_sentiment <- coffee %>%
  pivot_longer(cols = c(A, B, C, D),
               names_to = "document",
               values_to = "text") %>%
  unnest_tokens(word, text) %>%
  inner_join(get_sentiments("nrc"), by = "word") %>%
  filter(sentiment %in% c("positive", "negative")) %>%  # Keep only positive/negative categories for net sentiment
  count(review_id, document, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = list(n = 0)) %>%
  mutate(net_sentiment = positive - negative)

nrc_sentiment_with_rating <- nrc_sentiment %>%
  left_join(coffee %>% select(review_id, Rating), by = "review_id")
  ggplot(nrc_sentiment_with_rating, aes(x = net_sentiment, y = Rating)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~ document) +
  labs(title = "NRC Net Sentiment vs. Rating by Document",
       x = "Net NRC Sentiment (positive - negative)", y = "Rating") +
  theme_minimal()
```

```{r}
```