{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to scrap this website: https://www.coffeereview.com/review/\n",
    "\n",
    "Sitemap: https://www.coffeereview.com/sitemap_index.xml\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch URLs in the sitemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 422 review URLs.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get all review URLs from the sitemap\n",
    "sitemap_url = \"https://www.coffeereview.com/review-sitemap9.xml\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Fetch and parse the sitemap XML\n",
    "response = requests.get(sitemap_url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, \"xml\")\n",
    "    review_urls = [loc.text for loc in soup.find_all(\"loc\")]\n",
    "    print(f\"‚úÖ Found {len(review_urls)} review URLs.\")\n",
    "else:\n",
    "    print(f\"‚ùå Failed to fetch sitemap. Status code: {response.status_code}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.coffeereview.com/review/mocha-java-9/', 'https://www.coffeereview.com/review/guatemala-guayab-2/', 'https://www.coffeereview.com/review/guatemala-guayab-light-roast/', 'https://www.coffeereview.com/review/mayan-harvest-breakfast-blend-organic-fair-trade/', 'https://www.coffeereview.com/review/songbird-shadegrown-guatemalan-organic-fair-trade/']\n",
      "421\n"
     ]
    }
   ],
   "source": [
    "print(review_urls[:5])\n",
    "print(len(set(review_urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.coffeereview.com/review/\n",
      "1000 URLs need to be scrapped.\n"
     ]
    }
   ],
   "source": [
    "# For the first sitemap: \"https://www.coffeereview.com/review-sitemap.xml\", we need to remove the first element which is https://www.coffeereview.com/review/\n",
    "if sitemap_url == \"https://www.coffeereview.com/review-sitemap.xml\":\n",
    "    print(review_urls.pop(0))\n",
    "    print(f\"{len(review_urls)} URLs need to be scrapped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Before scrapping, check the number of URLs\n",
    "# if sitemap_url != \"https://www.coffeereview.com/review-sitemap9.xml\":\n",
    "#     assert len(review_urls) == 1000\n",
    "# else:\n",
    "#     assert len(review_urls) == 379 # NOTE: This number might change if more reviews are added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Function to scrape text from a review page\n",
    "def scrape_text(url):\n",
    "    \"\"\" Scrapes all text from a given coffee review URL \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Skipped {url} (status code {response.status_code})\")\n",
    "            return None\n",
    "\n",
    "        # Extract text using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        all_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "        # Include the URL in the text\n",
    "        full_text = f\"URL: {url}\\n\\n{all_text}\"\n",
    "        return full_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error scraping {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Scraping 1/422: https://www.coffeereview.com/review/jaguar-espresso-blend/\n",
      "‚úÖ Saved: coffee_reviews_text\\https___www_coffeereview_com_review_jaguar-espresso-blend_.txt\n",
      "üéØ All reviews scraped and saved!\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Save each review text using URL as the file name\n",
    "save_dir = \"coffee_reviews_text\"\n",
    "os.makedirs(save_dir, exist_ok=True)  # Create directory if not exists\n",
    "\n",
    "for i, url in enumerate(review_urls): \n",
    "    print(f\"üîÑ Scraping {i+1}/{len(review_urls)}: {url}\")\n",
    "\n",
    "    text_content = scrape_text(url)\n",
    "    if text_content:\n",
    "        # Sanitize URL for file name (replace special characters with `_`)\n",
    "        safe_filename = re.sub(r\"[^\\w\\-]\", \"_\", url)  # Keep only alphanumeric, `_`, and `-`\n",
    "        file_path = os.path.join(save_dir, f\"{safe_filename}.txt\")\n",
    "\n",
    "        # Save the text\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text_content)\n",
    "\n",
    "        print(f\"‚úÖ Saved: {file_path}\")\n",
    "\n",
    "    # Avoid overwhelming the server (polite scraping)\n",
    "    time.sleep(3)\n",
    "\n",
    "print(\"üéØ All reviews scraped and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore: Check the scrapped content from 1 webpage for 1 coffee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Request successful!\n"
     ]
    }
   ],
   "source": [
    "# Target webpage\n",
    "url = \"https://www.coffeereview.com/review/wilton-benitez-colombia-yellow-bourbon/\"\n",
    "\n",
    "# Set User-Agent to simulate a browser request\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Send the request\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check response status code\n",
    "if response.status_code == 200:\n",
    "    print(\"‚úÖ Request successful!\")\n",
    "else:\n",
    "    print(f\"‚ùå Request failed, status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File saved.\n"
     ]
    }
   ],
   "source": [
    "with open(\"coffee_review_html_code_sample.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "print(\"‚úÖ File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Plain text file saved as coffee_review.txt. You can open it with Notepad or VSCode.\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "all_text = soup.get_text(separator=\"\\n\", strip=True)  # Extract all text\n",
    "\n",
    "with open(\"coffee_review_text_sample.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(all_text)\n",
    "\n",
    "print(\"‚úÖ Plain text file saved as coffee_review.txt. You can open it with Notepad or VSCode.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
